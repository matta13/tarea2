version: '3.8' # Usar una versi贸n de compose que soporte KRaft

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.12
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - claudio
  
  #  SERVICIO KAFKA
  kafka:
    image: confluentinc/cp-kafka:7.4.12
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_started
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://kafka:9092,PLAINTEXT://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    volumes:
      # Volumen para persistir los logs y los metadatos de KRaft
      - kafka_data:/bitnami/kafka/data
    networks:
      - claudio
    # NOTA: En la primera ejecuci贸n, Kafka inicializar谩 el almacenamiento KRaft.
    # Si quieres una inicializaci贸n expl铆cita, puedes a帽adir un servicio init.

  postgres:
    # ... (Se mantiene igual) ...
    image: postgres:15
    container_name: postgres_db
    restart: unless-stopped
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      POSTGRES_DB: mydatabase
    ports:
      - "5432:5432"
    volumes:
      - ./postgres/01schema.sql:/docker-entrypoint-initdb.d/create_schema.sql
      - ./postgres/02data.sql:/docker-entrypoint-initdb.d/data.sql
      - postgres_data:/var/lib/postgresql/data
    networks:
      - claudio
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d mydatabase"]
      interval: 5s
      timeout: 3s
      retries: 15

  api:
    build: ./api
    container_name: qa_api
    restart: unless-stopped
    env_file:
      - .env.api
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_started
    ports:
      - "8000:8000"
    networks:
      - claudio
  
  # --- 4 Servicios para la Carga del LLM (Gemini Workers) ---
  # Se mantienen sin cambios, solo dependen de Kafka
  llm_producer_1:
    build: ./llm_processor
    container_name: llm_producer_1
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]
  llm_producer_2:
    build: ./llm_processor
    container_name: llm_producer_2
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  llm_producer_3:
    build: ./llm_processor
    container_name: llm_producer_3
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  llm_producer_4:
    build: ./llm_processor
    container_name: llm_producer_4
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  # --- Servicio para Guardar en DB (Storage Consumer) ---
  storage_consumer:
    build: ./storage_consumer
    container_name: qa_scorer
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "storage_scorer.py"]

  db_writer_consumer:
    build: ./db_writer # Nueva carpeta
    container_name: db_writer
    env_file:
      - .env.api
    volumes:
      # Montar main.py para usar la funci贸n de escritura a DB
      - ./api/main.py:/temp/main.py 
    depends_on:
      postgres:
        condition: service_healthy # Requiere la DB para escribir
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "db_writer_consumer.py"]

  qa_client:
    build: ./client
    container_name: qa_client
    restart: "no"
    environment:
      - API_URL=http://qa_api:8000/ask
    depends_on:
      - api
    stdin_open: true   
    tty: true
    networks:
      - claudio

networks:
  claudio:
    driver: bridge

volumes:
  postgres_data:
  kafka_data: # Nuevo volumen para KRaft
