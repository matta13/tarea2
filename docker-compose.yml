version: '3.8' # Usar una versión de compose que soporte KRaft

services:
  # Se elimina completamente el servicio zookeeper

  kafka:
    image: bitnamilegacy/kafka:4.0.0-debian-12-r10 # Nueva versión
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      # --- Configuración KRaft (Kafka sin Zookeeper) ---
      - KAFKA_CFG_NODE_ID=0 # ID del nodo (único para este nodo)
      - KAFKA_CFG_PROCESS_ROLES=controller,broker # Este nodo es controlador y broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # El Cluster ID se generará automáticamente o se debe pre-generar.
      # Para desarrollo, omitir KAFKA_CLUSTER_ID y dejar que Kafka lo genere.
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093 # Único votante/controlador
      
      # Evita problemas de réplica en un entorno de un solo nodo
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      
    volumes:
      # Volumen para persistir los logs y los metadatos de KRaft
      - kafka_data:/bitnami/kafka/data
    networks:
      - claudio
    # NOTA: En la primera ejecución, Kafka inicializará el almacenamiento KRaft.
    # Si quieres una inicialización explícita, puedes añadir un servicio init.

  postgres:
    # ... (Se mantiene igual) ...
    image: postgres:15
    container_name: postgres_db
    restart: unless-stopped
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      POSTGRES_DB: mydatabase
    ports:
      - "5432:5432"
    volumes:
      - ./postgres/01schema.sql:/docker-entrypoint-initdb.d/create_schema.sql
      - ./postgres/02data.sql:/docker-entrypoint-initdb.d/data.sql
      - postgres_data:/var/lib/postgresql/data
    networks:
      - claudio
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d mydatabase"]
      interval: 5s
      timeout: 3s
      retries: 15

  api:
    build: ./api
    container_name: qa_api
    restart: unless-stopped
    env_file:
      - .env.api
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_started
    ports:
      - "8000:8000"
    networks:
      - claudio
  
  # --- 4 Servicios para la Carga del LLM (Gemini Workers) ---
  # Se mantienen sin cambios, solo dependen de Kafka
  llm_producer_1:
    build: ./llm_processor
    container_name: llm_producer_1
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["sh", "-c", "sleep 15 && python /app/llm_consumer_producer.py"]
  llm_producer_2:
    build: ./llm_processor
    container_name: llm_producer_2
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  llm_producer_3:
    build: ./llm_processor
    container_name: llm_producer_3
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  llm_producer_4:
    build: ./llm_processor
    container_name: llm_producer_4
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "llm_consumer_producer.py"]

  # --- Servicio para Guardar en DB (Storage Consumer) ---
  storage_consumer:
    build: ./storage_consumer
    container_name: qa_scorer
    env_file:
      - .env.api
    depends_on:
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "storage_scorer.py"]

  db_writer_consumer:
    build: ./db_writer # Nueva carpeta
    container_name: db_writer
    env_file:
      - .env.api
    volumes:
      # Montar main.py para usar la función de escritura a DB
      - ./api/main.py:/temp/main.py 
    depends_on:
      postgres:
        condition: service_healthy # Requiere la DB para escribir
      kafka:
        condition: service_started
    networks:
      - claudio
    command: ["python", "db_writer_consumer.py"]

  qa_client:
    build: ./client
    container_name: qa_client
    restart: "no"
    environment:
      - API_URL=http://qa_api:8000/ask
    depends_on:
      - api
    stdin_open: true   
    tty: true
    networks:
      - claudio

networks:
  claudio:

volumes:
  postgres_data:
  kafka_data: # Nuevo volumen para KRaft